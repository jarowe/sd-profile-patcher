---
phase: 02-data-pipeline-privacy
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - pipeline/edges/edge-generator.mjs
  - pipeline/edges/signals.mjs
  - pipeline/layout/helix.mjs
  - pipeline/index.mjs
  - src/constellation/data/loader.js
autonomous: true
requirements: [PIPE-03, PIPE-04, PIPE-05, PRIV-09]

must_haves:
  truths:
    - "The constellation contains real projects, real blog posts, and real career milestones from Carbonmade data (and Instagram if available), with evidence-based edges showing why nodes are connected"
    - "Evidence-based edges are generated using the signal weight table (same-day 0.8, shared-project 0.7, shared-entity 0.6, shared-tags 0.4, temporal-proximity 0.3) with threshold >= 0.5"
    - "Each edge has an evidence array showing 'Because...' reasons with type, signal, description, and weight"
    - "The output JSON is a drop-in replacement for mock-constellation.json -- same node/edge shape, same epoch structure"
    - "Running the pipeline twice on the same input produces byte-identical output"
    - "The data loader fetches real data when available and falls back to mock data in dev"
  artifacts:
    - path: "pipeline/edges/edge-generator.mjs"
      provides: "Evidence-based edge generation with signal weights and pruning"
      exports: ["generateEdges"]
    - path: "pipeline/edges/signals.mjs"
      provides: "Signal weight constants and calculation functions"
      exports: ["SIGNAL_WEIGHTS", "EDGE_THRESHOLD", "calculateSignals"]
    - path: "pipeline/layout/helix.mjs"
      provides: "Double-helix layout for real pipeline data"
      exports: ["computePipelineLayout"]
    - path: "pipeline/index.mjs"
      provides: "Pipeline orchestrator (parse -> normalize -> connect -> layout -> emit)"
      exports: []
    - path: "src/constellation/data/loader.js"
      provides: "Data loader that fetches real data or falls back to mock"
      exports: ["loadConstellationData"]
  key_links:
    - from: "pipeline/index.mjs"
      to: "pipeline/parsers/instagram.mjs"
      via: "parseInstagram() call"
      pattern: "parseInstagram"
    - from: "pipeline/index.mjs"
      to: "pipeline/parsers/carbonmade.mjs"
      via: "parseCarbonmade() call"
      pattern: "parseCarbonmade"
    - from: "pipeline/index.mjs"
      to: "pipeline/edges/edge-generator.mjs"
      via: "generateEdges(allNodes) call"
      pattern: "generateEdges"
    - from: "pipeline/index.mjs"
      to: "pipeline/layout/helix.mjs"
      via: "computePipelineLayout(nodes) call"
      pattern: "computePipelineLayout"
    - from: "pipeline/index.mjs"
      to: "pipeline/privacy/exif-stripper.mjs"
      via: "stripAndVerify() call on each media file during privacy phase"
      pattern: "stripAndVerify"
    - from: "pipeline/index.mjs"
      to: "public/data/constellation.graph.json"
      via: "fs.writeFile with deterministicStringify"
      pattern: "constellation\\.graph\\.json"
    - from: "src/constellation/data/loader.js"
      to: "/data/constellation.graph.json"
      via: "fetch() at runtime"
      pattern: "fetch.*constellation"
---

<objective>
Build the pipeline orchestrator that runs end-to-end (parse -> connect -> layout -> emit) and produces constellation.graph.json + constellation.layout.json.

Purpose: This is the core pipeline assembly. It wires both parsers together, generates evidence-based edges between all nodes using the locked signal weight table, computes helix layout positions, and emits the final deterministic JSON files. It also creates the frontend data loader so components can consume real data.

Output: `pipeline/edges/edge-generator.mjs`, `pipeline/edges/signals.mjs`, `pipeline/layout/helix.mjs`, `pipeline/index.mjs`, `src/constellation/data/loader.js`, updated `package.json` scripts
</objective>

<execution_context>
@C:\Users\rowej\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\rowej\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-data-pipeline-privacy/02-CONTEXT.md
@.planning/phases/02-data-pipeline-privacy/02-RESEARCH.md
@.planning/phases/02-data-pipeline-privacy/02-01-SUMMARY.md
@.planning/phases/02-data-pipeline-privacy/02-02-SUMMARY.md
@src/constellation/data/mock-constellation.json
@src/constellation/layout/helixLayout.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Evidence-based edge generation and helix layout</name>
  <files>
    pipeline/edges/signals.mjs
    pipeline/edges/edge-generator.mjs
    pipeline/layout/helix.mjs
  </files>
  <action>
    Create `pipeline/edges/signals.mjs`:
    - Export SIGNAL_WEIGHTS object (LOCKED by user decision):
      - 'same-day': 0.8
      - 'shared-project': 0.7
      - 'shared-entity': 0.6
      - 'shared-tags': 0.4
      - 'temporal-proximity': 0.3
    - Export EDGE_THRESHOLD = 0.5 (LOCKED: total weight >= 0.5 to create edge)
    - Export `calculateSignals(nodeA, nodeB)`: Returns array of signal evidence objects. For each signal type:
      - **same-day**: nodeA.date === nodeB.date -> weight 0.8, evidence description "Both from {date}"
      - **shared-project**: intersection of entities.projects -> weight 0.7, description "Both part of {projects}"
      - **shared-entity**: intersection of entities.people (case-insensitive) -> weight 0.6, description "Both mention {people}"
      - **shared-tags**: intersection of entities.tags (case-insensitive) -> weight 0.4, description "Shared tags: {tags}"
      - **temporal-proximity**: |daysBetween| > 0 and <= 30 -> weight 0.3, description "{N} days apart"
      - **shared-place**: intersection of entities.places (case-insensitive) -> weight 0.25 (bonus signal, not in locked table but within discretion for evidence quality)
      - **shared-client**: intersection of entities.clients -> weight 0.35 (bonus signal for Carbonmade project connections)
    - Use date-fns differenceInDays for temporal calculation
    - Helper: `intersect(a, b)` case-insensitive array intersection

    Create `pipeline/edges/edge-generator.mjs`:
    - Export `generateEdges(nodes)`:
      - Sort nodes by id for deterministic pair ordering
      - For each unique pair (i, j where i < j), call calculateSignals(nodes[i], nodes[j])
      - Sum signal weights. If total >= EDGE_THRESHOLD, create edge.
      - Edge shape matches mock-constellation.json: { source, target, weight: Number(totalWeight.toFixed(2)), evidence: [...] }
      - **Pruning**: Keep top 6 edges per node per signal type (per PIPE-05). After generating all edges, for each node, if it has >6 edges of a single type, keep the 6 highest-weighted.
      - **Connections field**: After edge generation, populate each node's `connections` array with IDs of connected nodes (matching mock format).
      - Sort edges deterministically by source + target for byte-identical output.
      - Return { edges, stats: { totalPairs, edgesCreated, edgesPruned } }

    Create `pipeline/layout/helix.mjs`:
    - Export `computePipelineLayout(nodes, config)`:
      - Adapt from existing `src/constellation/layout/helixLayout.js` (copy the mulberry32 + computeHelixLayout logic, or import from pipeline/utils/deterministic.mjs for mulberry32).
      - Accept same config params: radius, pitch, epochGap, jitterRadius, seed (defaults from PIPELINE_CONFIG).
      - Output positions map: { [nodeId]: { x, y, z } } -- separate from node data for constellation.layout.json.
      - Also output helixParams and bounds for the layout JSON.
      - Return { positions, helixParams, bounds: { minY, maxY } }
  </action>
  <verify>
    Test edge generation with a small node set: `node -e "import('./pipeline/edges/edge-generator.mjs').then(m => { const nodes = [{id:'a',date:'2020-01-01',entities:{people:['Alice'],tags:['art'],places:[],clients:[],projects:[]},connections:[]},{id:'b',date:'2020-01-01',entities:{people:['Alice'],tags:['code'],places:[],clients:[],projects:[]},connections:[]}]; m.generateEdges(nodes).then(r => console.log(JSON.stringify(r.edges, null, 2))); })"` -- should produce edge with weight >= 0.5 (same-day 0.8 + shared-entity 0.6 = 1.4).
    Test layout produces valid positions: `node -e "import('./pipeline/layout/helix.mjs').then(m => { const nodes = [{id:'a',date:'2020-01-01',epoch:'Present',isHub:false,size:1},{id:'b',date:'2020-06-01',epoch:'Present',isHub:true,size:1.5}]; const r = m.computePipelineLayout(nodes); console.log(r.positions); })"` -- should print position objects with x, y, z.
  </verify>
  <done>
    Edge generator produces evidence-based edges with the locked signal weight table and >= 0.5 threshold. Pruning limits to top 6 per type per node. Layout produces deterministic helix positions. Both output sorted, deterministic results.
  </done>
</task>

<task type="auto">
  <name>Task 2: Pipeline orchestrator and frontend data loader</name>
  <files>
    pipeline/index.mjs
    src/constellation/data/loader.js
    package.json
  </files>
  <action>
    Create `pipeline/index.mjs` -- the main pipeline entry point:

    1. **Import all modules**: parsers, edge generator, layout, privacy utilities, config, logger, deterministic helpers.

    2. **Parse phase**:
       - Call parseInstagram(PIPELINE_CONFIG.sources.instagram.dir)
       - Call parseCarbonmade(PIPELINE_CONFIG.sources.carbonmade.dir)
       - Merge node arrays. Log source counts.
       - If total nodes === 0, log error and exit with code 1 (pipeline produced nothing).

    3. **Curation phase** (read curation.json if exists):
       - Load curation.json from PIPELINE_CONFIG.curation.file (this is a READ-ONLY input to the pipeline)
       - If curation.json exists, apply publish/hide overrides: nodes marked "hidden" are removed from output. Visibility overrides applied.
       - If curation.json doesn't exist, create a default one with all nodes visible.
       - Load allowlist.json from PIPELINE_CONFIG.allowlist.file for privacy module (used in Plan 04).
       - IMPORTANT: The pipeline NEVER writes back to curation.json. Runtime status (last run time, success/fail, node counts) is written to a SEPARATE file: `public/data/pipeline-status.json`. This separation ensures curation.json remains a clean version-controlled input file that only the admin UI modifies.

    4. **Privacy phase** (EXIF + GPS):
       - For each node with media files, call stripAndVerify() on each media file. Output to PIPELINE_CONFIG.output.mediaDir.
       - Update node media paths to point to stripped copies (relative to public/).
       - For each node with location, call redactGPS() based on visibility tier.
       - Skip EXIF processing if source media not available (log warning, keep empty media array).

    5. **Edge generation phase**:
       - Call generateEdges(allNodes)
       - Log edge stats (total pairs checked, edges created, edges pruned).

    6. **Layout phase**:
       - Call computePipelineLayout(allNodes, PIPELINE_CONFIG.layout)
       - Log layout stats (position count, bounds).

    7. **Emit phase**:
       - Build graph JSON: { nodes (sorted by id), edges (sorted by source+target), epochs (from getEpochConfig()) }
       - Build layout JSON: { positions, helixParams, bounds }
       - Ensure output directory exists (mkdir -p public/data/)
       - Write both files using deterministicStringify()
       - Write pipeline-status.json with: { lastRun: ISO timestamp, status: "success", stats: { nodeCount, edgeCount, bySource, byType, byVisibility } }
       - Log file sizes and node/edge counts.

    8. **Summary**: Print pipeline summary with timing and counts. Exit with code 0 on success.

    IMPORTANT per user decision (determinism):
    - Sort ALL arrays before serialization (nodes by id, edges by source+target)
    - Use deterministicStringify for ALL output files
    - No timestamps in constellation data output (timestamps only in pipeline-status.json which is a runtime artifact)
    - No Math.random() anywhere -- use mulberry32 with seed

    Add to package.json scripts:
    ```json
    "pipeline": "node pipeline/index.mjs",
    "prebuild": "node pipeline/index.mjs"
    ```
    Note: prebuild runs automatically before `npm run build` (Vite build).

    Create `src/constellation/data/loader.js`:
    - Export `loadConstellationData()` async function:
      1. Try to fetch('/data/constellation.graph.json') and fetch('/data/constellation.layout.json')
      2. If both succeed, merge layout positions into node data (add x, y, z to each node from positions map)
      3. If fetch fails (404 in dev, no pipeline run), fall back to importing mock-constellation.json + computing layout via helixLayout.js
      4. Return { nodes (with positions), edges, epochs } -- same shape as mock data but with positions
    - This allows all 7 components to switch from `import mockData` to `const data = await loadConstellationData()` without structural changes.
    - Export the function as default AND named export for flexibility.
    - IMPORTANT: Do NOT modify the 7 consuming components yet. That migration should be done when real data is ready and the pipeline has been verified. The loader is created now so the interface is ready.
  </action>
  <verify>
    Run `node pipeline/index.mjs` -- should complete without errors (may produce minimal output if Instagram export not available, but Carbonmade data should produce ~70+ nodes).
    Verify output files exist: `ls public/data/constellation.graph.json public/data/constellation.layout.json public/data/pipeline-status.json`
    Verify output is valid JSON: `node -e "JSON.parse(require('fs').readFileSync('public/data/constellation.graph.json','utf8'))"` (or equivalent ESM).
    Verify pipeline-status.json has lastRun, status, and stats fields.
    Verify determinism: Run `node pipeline/index.mjs` twice and diff constellation outputs -- should be identical (pipeline-status.json will differ due to timestamps, that is expected).
    Verify data loader module loads: `node -e "import('./src/constellation/data/loader.js').then(m => console.log(typeof m.loadConstellationData))"` -- should print "function".
    Verify package.json has "pipeline" and "prebuild" scripts.
  </verify>
  <done>
    Pipeline runs end-to-end producing constellation.graph.json and constellation.layout.json from real data, plus pipeline-status.json with runtime metadata. Curation.json is read-only input (never written by pipeline). Edge generation uses locked signal weights with evidence arrays. Output is deterministic (byte-identical across runs for constellation data). Frontend data loader is ready for component migration. Pipeline runs via `npm run pipeline` and automatically before `npm run build`.
  </done>
</task>

</tasks>

<verification>
- `node pipeline/index.mjs` completes with exit code 0
- `public/data/constellation.graph.json` contains nodes, edges, epochs arrays
- `public/data/constellation.layout.json` contains positions, helixParams, bounds
- `public/data/pipeline-status.json` contains lastRun, status, stats (separate from curation.json)
- Edges have evidence arrays with type, signal, description, weight fields
- Output JSON nodes match mock-constellation.json shape (id, type, title, date, epoch, description, media, connections, size, isHub)
- Running pipeline twice produces identical constellation output (diff returns no differences)
- `npm run pipeline` works as a script command
</verification>

<success_criteria>
Complete pipeline produces valid constellation JSON from real Carbonmade data (and Instagram if available). Evidence-based edges use the locked signal weight table. Output is deterministic and matches the mock-constellation.json schema for drop-in replacement. Pipeline status is written to pipeline-status.json (NOT curation.json). Frontend data loader bridges the gap between pipeline output and component consumption.
</success_criteria>

<output>
After completion, create `.planning/phases/02-data-pipeline-privacy/02-03-SUMMARY.md`
</output>
